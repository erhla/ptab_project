{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv('standardized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(\n",
    "    sequential=True,\n",
    "    fix_length=500,\n",
    "    tokenize='spacy',\n",
    "    pad_first=True,\n",
    "    lower=True\n",
    ")\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float,\n",
    "                        use_vocab=False, \n",
    "                        sequential=False,\n",
    "                        is_target=True)\n",
    "\n",
    "\n",
    "full = data.TabularDataset('standardized.csv', 'csv', skip_header=True,\n",
    "        fields=[\n",
    "            ('docket', None),\n",
    "            ('outcome', None),\n",
    "            ('facts', TEXT),\n",
    "            ('conclusion', None),\n",
    "            ('target', LABEL)\n",
    "        ])\n",
    "\n",
    "TEXT.build_vocab(\n",
    "    full,\n",
    "    max_size=20000,\n",
    "    min_freq=10,\n",
    "    vectors=None\n",
    ")\n",
    "\n",
    "LABEL.build_vocab(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'facts': ['  ', 'the', 'subject', 'property', 'consists', 'of', 'a', 'one', '-', 'story', 'commercial', 'building', 'of', 'masonry', 'exterior', 'construction', 'that', 'was', 'built', 'in', '1995', '.', ' ', 'the', 'docket', 'no', ':', '12', '-', '00035.001-c-2', '   ', '2', 'of', '5', 'building', 'contains', '6,040', 'square', 'feet', 'of', 'building', 'area', 'with', 'a', '344', 'square', 'foot', 'canopy', '.', ' ', 'the', 'building', 'is', 'situated', 'on', '33,000', 'square', 'feet', 'of', 'land', 'area', '.', ' ', 'the', 'subject', 'property', 'is', 'a', 'class', 'a', 'office', 'building', 'used', 'as', 'a', 'branch', 'banking', 'facility', '.', ' ', 'the', 'subject', 'property', 'is', 'located', 'in', 'champaign', 'township', ',', 'champaign', 'county', ',', 'illinois', '    ', 'the', 'appellant', 'contends', 'overvaluation', 'as', 'the', 'basis', 'of', 'the', 'appeal', '.', ' ', 'in', 'support', 'of', 'this', 'argument', ',', 'the', 'appellant', 'submitted', 'limited', 'information', 'on', 'three', 'suggested', 'comparable', 'sales', '.', ' ', 'the', 'comparables', 'consist', 'of', 'one', '-', 'story', 'buildings', 'of', 'masonry', 'construction', 'that', 'range', 'in', 'size', 'from', '3,000', 'to', '6,618', 'square', 'feet', 'of', 'building', 'area', '.', ' ', 'comparables', '#', '1', 'and', '#', '2', 'were', 'built', 'in', '2000', 'and', '2005', 'while', 'the', 'age', 'of', 'comparable', '#', '3', 'was', 'not', 'disclosed', '.', ' ', 'the', 'appellant', 'indicated', 'the', 'comparables', 'sold', 'from', 'april', '2009', 'to', 'february', '2011', 'for', 'reported', 'prices', 'from', '$', '82', 'to', '$', '139', 'per', 'square', 'foot', 'of', 'building', 'area', 'including', 'land', '.', ' ', 'however', ',', 'the', 'appellant', 'did', 'not', 'disclose', 'the', 'comparables', \"'\", 'actual', 'sale', 'prices', 'or', 'submit', 'any', 'corroborating', 'documentation', 'regarding', 'these', 'purported', 'sales', '.', '   ', 'the', 'appellant', 'also', 'submitted', 'the', 'final', 'decision', 'issued', 'by', 'the', 'champaign', 'county', 'board', 'of', 'review', 'disclosing', 'the', 'subject', \"'s\", 'final', 'assessment', 'of', '$', '384,470', '.', ' ', 'the', 'subject', \"'s\", 'assessment', 'reflects', 'an', 'estimated', 'market', 'value', 'of', '$', '1,153,525', 'when', 'applying', 'champaign', 'county', \"'s\", '2012', 'three', '-', 'year', 'average', 'median', 'level', 'of', 'assessment', 'of', '33.33', '%', 'as', 'determined', 'by', 'the', 'department', 'of', 'revenue', '.', '86', 'ill.admin.code', 'ยง', '1910.50(c)(1', ')', '.', ' ', 'based', 'on', 'this', 'evidence', ',', 'the', 'appellant', 'requested', 'the', 'subject', \"'s\", 'assessment', 'be', 'reduced', 'to', '$', '241,575', ',', 'which', 'reflects', 'an', 'estimated', 'market', 'value', 'of', 'approximately', '$', '724,725', '.', '   ', 'the', 'board', 'of', 'review', 'did', 'not', 'submit', 'its', '\"', 'board', 'of', 'review', 'notes', 'on', 'appeal', '\"', 'or', 'any', 'evidence', 'in', 'support', 'of', 'its', 'assessment', 'of', 'the', 'subject', 'property', 'as', 'required', 'by', 'section', '1910.40(a', ')', 'of', 'the', 'rules', 'of', 'the', 'property', 'tax', 'appeal', 'board', '.', '86', 'ill.admin.code', 'ยง', '1910.40(a', ')', '.', ' ', 'thus', ',', 'the', 'board', 'of', 'review', 'was', 'found', 'to', 'be', 'in', 'default', 'pursuant', 'to', 'section', '1910.69(a', ')', 'of', 'the', 'rules', 'of', 'the', 'property', 'tax', 'appeal', 'board', '.', '86', 'ill.admin.code', 'ยง', '1910.69(a', ')', '.', '   ', 'the', 'city', 'of', 'champaign', ',', 'a', 'taxing', 'body', 'with', 'a', 'revenue', 'interest', 'in', 'the', 'subject', 'property', ',', 'timely', 'filed', 'intervention', 'in', 'this', 'matter', '.', ' ', 'in', 'support', 'of', 'the', 'subject', \"'s\", 'correct', 'assessment', ',', 'the', 'intervenor', 'submitted', 'an', 'appraisal', 'estimating', 'the', 'subject', 'property', 'had', 'a', 'market', 'value', 'of', '$', '950,000', 'as', 'of', 'october', '9', ',', '2013', '.', ' ', 'the', 'appraiser', 'developed', 'two', 'of', 'the', 'three', 'traditional', 'approaches', 'to', 'value', 'in', 'arriving', 'at', 'the', 'final', 'value', 'conclusion', '.', '  ', 'docket', 'no', ':', '12', '-', '00035.001-c-2', '   ', '3', 'of', '5'], 'target': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(full.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED= 189053\n",
    "\n",
    "train_data, test_data, valid_data = full.split(split_ratio=[0.7, 0.15, 0.15], random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 31921\n",
      "Number of validation examples: 6840\n",
      "Number of testing examples: 6841\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 17249\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Return accuracy per batch\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "class WordEmbAvg(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define an embedding layer, a couple of linear layers, and \n",
    "        # the ReLU non-linearity.\n",
    "\n",
    "        ##YOUR CODE HERE##\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)  \n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "                                 \n",
    "        \n",
    "    def forward(self, text):\n",
    "        embeddings = self.embedding(text)\n",
    "        embeddings_avg = embeddings.mean(0)\n",
    "        output = self.linear1(embeddings_avg)\n",
    "        final = self.linear2(self.relu(output))\n",
    "        return final\n",
    "    \n",
    "class Training_module( ):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "       \n",
    "       ##YOUR CODE HERE##\n",
    "       # Choose an optimizer. optim.Adam is a popular choice\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.model.train()\n",
    "\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        '''\n",
    "        Train the model for one epoch. For this repeat the following, \n",
    "        going through all training examples.\n",
    "        1. Get the next batch of inputs from the iterator.\n",
    "        2. Determine the predictions using a forward pass.\n",
    "        3. Compute the loss.\n",
    "        4. Compute gradients using a backward pass.\n",
    "        5. Execute one step of the optimizer to update the model paramters.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        for batch in iterator:\n",
    "          # batch.facts has the texts and batch.target has the labels.          \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            ##YOUR CODE HERE##\n",
    "            predictions = self.model(batch.facts).squeeze(1)\n",
    "            loss = self.loss_fn(predictions, batch.target)                      \n",
    "            accuracy = binary_accuracy(predictions, batch.target) \n",
    "                   \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "                        \n",
    "            #print(accuracy.item())\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy.item()\n",
    "            \n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def train_model(self, train_iterator, dev_iterator):\n",
    "        \"\"\"\n",
    "        Train the model for multiple epochs, and after each evaluate on the\n",
    "        development set.  Return the best performing model.\n",
    "        \"\"\"  \n",
    "        dev_accs = [0.]\n",
    "        for epoch in range(5):\n",
    "            self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(f\"Epoch {epoch}: Dev Accuracy: {dev_acc[1]} Dev Loss:{dev_acc[0]}\")\n",
    "            if dev_acc[1] > max(dev_accs):\n",
    "                best_model = copy.deepcopy(self)\n",
    "            dev_accs.append(dev_acc[1])\n",
    "        return best_model.model\n",
    "                \n",
    "    def evaluate(self, iterator):\n",
    "        '''\n",
    "        Evaluate the performance of the model on the given examples.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            print(iterator)\n",
    "            for batch in iterator:\n",
    "                print(batch)\n",
    "                print(self.model(batch.facts))\n",
    "                predictions = self.model(batch.facts).squeeze(1)\n",
    "                loss = self.loss_fn(predictions, batch.target)                      \n",
    "                accuracy = binary_accuracy(predictions, batch.target) \n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += accuracy.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "#You can try many different embedding dimensions. Common values are 20, 32, 64, 100, 128, 512\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "#Get the index of the pad token using the stoi function\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "\n",
    "model = WordEmbAvg(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.iterator.BucketIterator object at 0x00000264579A3488>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Example' and 'Example'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-2aedf27547ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Training the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-105-8461403e5e48>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, train_iterator, dev_iterator)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mdev_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch}: Dev Accuracy: {dev_acc[1]} Dev Loss:{dev_acc[0]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdev_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_accs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-105-8461403e5e48>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfacts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[1;31m# fast-forward if loaded from state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\iterator.py\u001b[0m in \u001b[0;36minit_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_random_state_this_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restored_from_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\iterator.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             self.batches = batch(self.data(), self.batch_size,\n\u001b[0m\u001b[0;32m    246\u001b[0m                                  self.batch_size_fn)\n\u001b[0;32m    247\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\iterator.py\u001b[0m in \u001b[0;36mdata\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;34m\"\"\"Return the examples in the dataset in order, sorted, or shuffled.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'Example' and 'Example'"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "tm = Training_module(model)\n",
    "\n",
    "#Training the model\n",
    "best_model = tm.train_model(train_iterator, valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.model = best_model\n",
    "test_loss, test_acc = tm.evaluate(test_iterator)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
