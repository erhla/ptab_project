{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#based on homework 2 framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('standardized.csv')\n",
    "decisions = pd.read_csv('decisions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(\n",
    "    sequential=True,\n",
    "    fix_length=1000,\n",
    "    tokenize='spacy',\n",
    "    pad_first=True,\n",
    "    lower=True\n",
    ")\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float,\n",
    "                        use_vocab=False, \n",
    "                        sequential=False,\n",
    "                        is_target=True)\n",
    "\n",
    "full = data.TabularDataset('standardized.csv', 'csv', skip_header=True,\n",
    "        fields=[\n",
    "            ('docket', None),\n",
    "            ('outcome', None),\n",
    "            ('facts', TEXT),\n",
    "            ('conclusion', None),\n",
    "            ('target', LABEL)\n",
    "        ])\n",
    "\n",
    "TEXT.build_vocab(\n",
    "    full,\n",
    "    max_size=20000,\n",
    "    min_freq=150,\n",
    "    vectors=None\n",
    ")\n",
    "\n",
    "LABEL.build_vocab(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'facts': ['  ', 'the', 'subject', 'property', 'consists', 'of', 'a', 'split', '-', 'level', 'dwelling', 'of', 'frame', 'exterior', 'construction', 'with', '925', 'square', 'feet', 'of', 'living', 'area', '.', ' ', 'the', 'dwelling', 'was', 'constructed', 'in', '1973', '.', ' ', 'features', 'of', 'the', 'home', 'include', 'a', 'finished', 'lower', 'level', 'and', 'a', '400', 'square', 'foot', 'garage', '.', ' ', 'the', 'property', 'has', 'a', '5,480', 'square', 'foot', 'site', 'and', 'is', 'located', 'in', 'round', 'lake', 'beach', ',', 'avon', 'township', ',', 'lake', 'county', '.', ' ', 'the', 'appellants', 'contend', 'overvaluation', 'as', 'the', 'basis', 'of', 'the', 'appeal', '.', ' ', 'in', 'support', 'of', 'this', 'argument', 'the', 'appellant', 'submitted', 'information', 'on', 'three', 'comparable', 'sales', 'located', 'from', '1.82', 'to', '2.42', 'miles', 'from', 'the', 'subject', '.', ' ', 'the', 'comparables', 'consist', 'of', 'dwellings', 'of', 'frame', 'exterior', 'construction', 'with', 'the', 'same', 'design', 'code', 'as', 'the', 'subject', 'with', 'either', '864', 'or', '925', 'square', 'feet', 'of', 'living', 'area', '.', ' ', 'the', 'dwellings', 'were', 'either', '46', 'or', '53', 'years', 'old', '.', ' ', 'two', 'comparables', 'have', 'central', 'air', 'conditioning', 'and', 'one', 'comparable', 'has', 'a', 'fireplace', '.', ' ', 'the', 'comparables', 'have', 'sites', 'ranging', 'in', 'size', 'from', '4,200', 'to', '6,970', 'square', 'feet', 'of', 'land', 'area', '.', ' ', 'the', 'comparables', 'sold', 'from', 'march', '2015', 'to', 'february', '2016', 'for', 'prices', 'ranging', 'from', 'docket', 'no', ':', '16', '-', '02525.001-r-1', '   ', '2', 'of', '5', '$', '43,000', 'to', '$', '64,000', 'or', 'from', '$', '49.77', 'to', '$', '69.19', 'per', 'square', 'foot', 'of', 'living', 'area', ',', 'including', 'land', '.', ' ', 'based', 'on', 'this', 'evidence', ',', 'the', 'appellants', 'requested', 'a', 'reduction', 'in', 'the', 'subject', '™', 's', 'assessment', '.', ' ', 'the', 'board', 'of', 'review', 'submitted', 'its', '\"', 'board', 'of', 'review', 'notes', 'on', 'appeal', '\"', 'disclosing', 'the', 'total', 'assessment', 'for', 'the', 'subject', 'of', '$', '25,574', '.', ' ', 'the', 'subject', \"'s\", 'assessment', 'reflects', 'a', 'market', 'value', 'of', '$', '77,123', 'or', '$', '83.38', 'per', 'square', 'foot', 'of', 'living', 'area', ',', 'land', 'included', ',', 'when', 'using', 'the', '2016', 'three', '-', 'year', 'average', 'median', 'level', 'of', 'assessment', 'for', 'lake', 'county', 'of', '33.16', '%', 'as', 'determined', 'by', 'the', 'illinois', 'department', 'of', 'revenue', '.', ' ', 'in', 'support', 'of', 'its', 'contention', 'of', 'the', 'correct', 'assessment', ',', 'the', 'board', 'of', 'review', 'submitted', 'information', 'on', 'four', 'comparable', 'sales', 'located', 'from', '.172', 'to', '.816', 'of', 'a', 'mile', 'from', 'the', 'subject', 'property', '.', ' ', 'the', 'comparables', 'were', 'improved', 'with', 'split', '-', 'level', 'dwellings', 'of', 'frame', 'exterior', 'construction', 'ranging', 'in', 'size', 'from', '925', 'to', '1,053', 'square', 'feet', 'of', 'living', 'area', '.', ' ', 'the', 'dwellings', 'were', 'constructed', 'from', '1971', 'to', '1974', '.', ' ', 'each', 'comparable', 'has', 'a', 'finished', 'lower', 'level', ',', 'three', 'comparables', 'have', 'central', 'air', 'conditioning', 'and', 'one', 'comparable', 'has', 'a', 'fireplace', '.', ' ', 'the', 'comparables', 'each', 'have', 'a', 'garage', 'ranging', 'in', 'size', 'from', '440', 'to', '624', 'square', 'feet', 'of', 'building', 'area', '.', ' ', 'the', 'comparables', 'have', 'sites', 'ranging', 'in', 'size', 'from', '5,201', 'to', '10,500', 'square', 'feet', 'of', 'land', 'area', '.', ' ', 'these', 'properties', 'sold', 'in', 'march', '2015', 'or', 'may', '2015', 'for', 'prices', 'ranging', 'from', '$', '81,000', 'to', '$', '143,000', 'or', 'from', '$', '76.92', 'to', '$', '154.59', 'per', 'square', 'foot', 'of', 'living', 'area', ',', 'including', 'land', '.', ' ', 'based', 'on', 'this', 'evidence', ',', 'the', 'board', 'of', 'review', 'requested', 'confirmation', 'of', 'the', 'subject', '™', 's', 'assessment', '.', ' '], 'target': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(full.examples[38000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 189053\n",
    "train_data, test_data, valid_data = full.split(split_ratio=[0.7, 0.15, 0.15], random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 31921\n",
      "Number of validation examples: 6840\n",
      "Number of testing examples: 6841\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 2173\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "#13114 unique @ min_freq 15\n",
    "#8700 unique @ min_freq 25\n",
    "#6186 unique @ min_freq 35\n",
    "#4533 unique @ min_freq 50\n",
    "#3352 unique @ min_freq 75\n",
    "#2799 unique @ min_freq 100\n",
    "#2173 unique @ min_freq 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = 'cpu',\n",
    "    sort_key=lambda x: len(x.facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Return accuracy per batch\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "class WordEmbAvg(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)  \n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()                                 \n",
    "        \n",
    "    def forward(self, text):\n",
    "        embeddings = self.embedding(text)\n",
    "        embeddings_avg = embeddings.mean(0)\n",
    "        output = self.linear1(embeddings_avg)\n",
    "        final = self.linear2(self.relu(output))\n",
    "        return final\n",
    "    \n",
    "class TrainingModule():\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.model.train()\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        for batch in iterator:\n",
    "            #batch.facts has the texts and batch.target has the labels.          \n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(batch.facts).squeeze(1)\n",
    "            loss = self.loss_fn(predictions, batch.target)                      \n",
    "            accuracy = binary_accuracy(predictions, batch.target) \n",
    "                   \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "                        \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def train_model(self, train_iterator, dev_iterator):\n",
    "        dev_accs = [0.]\n",
    "        for epoch in range(5):\n",
    "            self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(f\"Epoch {epoch}: Dev Accuracy: {dev_acc[1]} Dev Loss:{dev_acc[0]}\")\n",
    "            if dev_acc[1] > max(dev_accs):\n",
    "                best_model = copy.deepcopy(self)\n",
    "            dev_accs.append(dev_acc[1])\n",
    "        return best_model.model\n",
    "                \n",
    "    def evaluate(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                predictions = self.model(batch.facts).squeeze(1)\n",
    "                loss = self.loss_fn(predictions, batch.target)                      \n",
    "                accuracy = binary_accuracy(predictions, batch.target) \n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += accuracy.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = WordEmbAvg(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Dev Accuracy: 0.836552904030987 Dev Loss:0.3811733804852049\n",
      "Epoch 1: Dev Accuracy: 0.8514060411497811 Dev Loss:0.34807823500900625\n",
      "Epoch 2: Dev Accuracy: 0.856892523364486 Dev Loss:0.33633933606270316\n",
      "Epoch 3: Dev Accuracy: 0.8658419557820971 Dev Loss:0.3261891924471499\n",
      "Epoch 4: Dev Accuracy: 0.8694717956480579 Dev Loss:0.31996052183001955\n"
     ]
    }
   ],
   "source": [
    "model = model.to()\n",
    "tm = TrainingModule(model)\n",
    "\n",
    "best_model = tm.train_model(train_iterator, valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.325 | Test Acc: 86.21%\n"
     ]
    }
   ],
   "source": [
    "tm.model = best_model\n",
    "test_loss, test_acc = tm.evaluate(test_iterator)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tm.model.embedding.weight.data\n",
    "result = list(torch.norm(a, p=2, dim=1).numpy())\n",
    "\n",
    "word_ls = TEXT.vocab.itos\n",
    "top_n = 20\n",
    "top_neg = {}\n",
    "top_pos = {}\n",
    "\n",
    "max_pos = [result.index(x) for x in sorted(result, reverse=True)][:top_n]\n",
    "max_neg = [result.index(x) for x in sorted(result)][:top_n]\n",
    "\n",
    "for i in range(top_n):\n",
    "    i_p = max_pos[i]\n",
    "    i_n = max_neg[i]\n",
    "    top_pos[word_ls[i_p]] = result[i_p]\n",
    "    top_neg[word_ls[i_n]] = result[i_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0.0,\n",
       " 'front': 12.02051,\n",
       " 'fullbasement': 12.048797,\n",
       " '125,000': 12.0652485,\n",
       " 'types': 12.140268,\n",
       " '24': 12.229512,\n",
       " 'pleadings': 12.2781,\n",
       " 'estimates': 12.288643,\n",
       " '3,125': 12.36296,\n",
       " 'parcels': 12.36658,\n",
       " 'dwellingswere': 12.431138,\n",
       " 'comps': 12.4603,\n",
       " '77': 12.502497,\n",
       " '13': 12.50694,\n",
       " 'collection': 12.520863,\n",
       " 'an': 12.545551,\n",
       " 'subarea': 12.576534,\n",
       " 'rooms': 12.58088,\n",
       " 'public': 12.593324,\n",
       " 'includes': 12.609003}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'limited': 19.90613,\n",
       " 'foreclosure': 19.026604,\n",
       " 'fourequity': 18.132935,\n",
       " 'proposed': 17.663214,\n",
       " 'valuation': 17.593716,\n",
       " 'timely': 17.452204,\n",
       " 'correctly': 17.370247,\n",
       " 'bases': 17.180614,\n",
       " 'townhome': 17.082584,\n",
       " 'submit': 17.054323,\n",
       " 'establishing': 16.894701,\n",
       " 'e': 16.739061,\n",
       " 'expense': 16.662413,\n",
       " 'frankfort': 16.6024,\n",
       " '8.94': 16.593983,\n",
       " '1900': 16.55477,\n",
       " 'aurora': 16.547735,\n",
       " 'agreed': 16.519125,\n",
       " 'substantive': 16.513819,\n",
       " 'insupport': 16.50307}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternate Evaluation Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pd.merge(data_raw, decisions, left_on='docket', right_on='docket_name')\n",
    "joined['year'] = joined.date.str.slice(0,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_subset(joined, attr, val):\n",
    "    bool_list = joined[attr] == val\n",
    "    bool_list = bool_list.tolist()\n",
    "    filtered_full = [i for (i, val) in zip(full.examples, bool_list) if val]\n",
    "\n",
    "    mini_full = data.Dataset(\n",
    "        filtered_full,\n",
    "        fields=[('facts', TEXT),('target', LABEL)])\n",
    "\n",
    "    filtered_iterator = data.Iterator(\n",
    "        mini_full, \n",
    "        batch_size = BATCH_SIZE,\n",
    "        device = 'cpu',\n",
    "        sort_key=lambda x: len(x.facts))\n",
    "\n",
    "    test_loss, test_acc = tm.evaluate(filtered_iterator)\n",
    "    print(\"~~~~\")\n",
    "    print(\"Analyzing data where {} is {}\".format(attr, val))\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "    print(f'Number of cases: {joined[bool_list].shape[0]}')\n",
    "    print(f'Percent of cases with reductions: {joined[bool_list].target.mean()*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4166429244559381"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where county is Cook\n",
      "Test Loss: 0.286 | Test Acc: 88.38%\n",
      "Number of cases: 36073\n",
      "Percent of cases with reductions: 44.75%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'county', 'Cook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where appellant is Mack Companies\n",
      "Test Loss: 0.256 | Test Acc: 91.21%\n",
      "Number of cases: 281\n",
      "Percent of cases with reductions: 2.14%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'appellant', 'Mack Companies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where appellant is Inverclyde, LLC\n",
      "Test Loss: 0.422 | Test Acc: 82.03%\n",
      "Number of cases: 128\n",
      "Percent of cases with reductions: 37.50%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'appellant', 'Inverclyde, LLC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where county is Lake\n",
      "Test Loss: 0.378 | Test Acc: 86.05%\n",
      "Number of cases: 3487\n",
      "Percent of cases with reductions: 14.11%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'county', 'Lake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where reason_code is 2\n",
      "Test Loss: 0.310 | Test Acc: 87.28%\n",
      "Number of cases: 42338\n",
      "Percent of cases with reductions: 41.70%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'reason_code', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where reason_code is 1\n",
      "Test Loss: 0.363 | Test Acc: 83.70%\n",
      "Number of cases: 3195\n",
      "Percent of cases with reductions: 40.85%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'reason_code', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where prop_type is R\n",
      "Test Loss: 0.318 | Test Acc: 86.92%\n",
      "Number of cases: 41491\n",
      "Percent of cases with reductions: 37.86%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'prop_type', 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where prop_type is C\n",
      "Test Loss: 0.278 | Test Acc: 87.18%\n",
      "Number of cases: 3230\n",
      "Percent of cases with reductions: 79.13%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'prop_type', 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where prop_type is I\n",
      "Test Loss: 0.238 | Test Acc: 91.26%\n",
      "Number of cases: 847\n",
      "Percent of cases with reductions: 83.12%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'prop_type', 'I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where valuation_class is 1\n",
      "Test Loss: 0.313 | Test Acc: 87.07%\n",
      "Number of cases: 45125\n",
      "Percent of cases with reductions: 41.40%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'valuation_class', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where valuation_class is 2\n",
      "Test Loss: 0.405 | Test Acc: 80.48%\n",
      "Number of cases: 370\n",
      "Percent of cases with reductions: 60.27%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'valuation_class', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where year is 2016\n",
      "Test Loss: 0.304 | Test Acc: 87.86%\n",
      "Number of cases: 10473\n",
      "Percent of cases with reductions: 50.55%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'year', '2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where year is 2017\n",
      "Test Loss: 0.318 | Test Acc: 86.71%\n",
      "Number of cases: 8293\n",
      "Percent of cases with reductions: 38.02%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'year', '2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where year is 2018\n",
      "Test Loss: 0.285 | Test Acc: 88.62%\n",
      "Number of cases: 7399\n",
      "Percent of cases with reductions: 30.42%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'year', '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where year is 2019\n",
      "Test Loss: 0.323 | Test Acc: 87.12%\n",
      "Number of cases: 7490\n",
      "Percent of cases with reductions: 24.43%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'year', '2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~\n",
      "Analyzing data where year is 2020\n",
      "Test Loss: 0.358 | Test Acc: 85.61%\n",
      "Number of cases: 2721\n",
      "Percent of cases with reductions: 22.79%\n"
     ]
    }
   ],
   "source": [
    "eval_subset(joined, 'year', '2020')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
