{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('standardized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(\n",
    "    sequential=True,\n",
    "    fix_length=500,\n",
    "    tokenize='spacy',\n",
    "    pad_first=True,\n",
    "    lower=True\n",
    ")\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float,\n",
    "                        use_vocab=False, \n",
    "                        sequential=False,\n",
    "                        is_target=True)\n",
    "\n",
    "full = data.TabularDataset('standardized.csv', 'csv', skip_header=True,\n",
    "        fields=[\n",
    "            ('docket', None),\n",
    "            ('outcome', None),\n",
    "            ('facts', TEXT),\n",
    "            ('conclusion', None),\n",
    "            ('target', LABEL)\n",
    "        ])\n",
    "\n",
    "TEXT.build_vocab(\n",
    "    full,\n",
    "    max_size=20000,\n",
    "    min_freq=50,\n",
    "    vectors=None\n",
    ")\n",
    "\n",
    "LABEL.build_vocab(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'facts': ['  ', 'the', 'subject', 'property', 'consists', 'of', 'a', '6,250', 'parcel', 'of', 'land', 'improved', 'with', 'a', '95-year', 'old', ',', 'one', '-', 'story', ',', 'commercial', 'building', 'containing', '3,178', 'square', 'feet', 'of', 'building', 'area', '.', ' ', 'the', 'property', 'is', 'located', 'in', 'chicago', ',', 'jefferson', 'township', ',', 'cook', 'county', '.', ' ', 'the', 'property', 'is', 'a', 'docket', 'no', ':', '09', '-', '24022.001-c-1', '   ', '2', 'of', '4', 'class', '5', 'property', 'under', 'the', 'cook', 'county', 'real', 'property', 'assessment', 'classification', 'ordinance', '.', '  ', 'the', 'appellant', 'contends', 'overvaluation', 'as', 'the', 'basis', 'of', 'the', 'appeal', '.', 'in', 'support', 'of', 'this', 'argument', 'the', 'appellant', 'submitted', 'an', 'appraisal', 'estimating', 'the', 'subject', 'property', 'had', 'a', 'market', 'value', 'of', '$', '238,000', 'as', 'of', 'january', '1', ',', '2009', '.', '  ', 'the', 'board', 'of', 'review', 'submitted', 'its', '\"', 'board', 'of', 'review', 'notes', 'on', 'appeal', '\"', 'disclosing', 'the', 'total', 'assessment', 'for', 'the', 'subject', 'of', '$', '83,421', '.', ' ', 'the', 'subject', \"'s\", 'assessment', 'reflects', 'a', 'market', 'value', 'of', '$', '333,684', 'using', 'the', 'cook', 'county', 'ordinance', 'level', 'of', 'assessment', 'for', 'class', '5', 'property', 'of', '25', '%', '.', ' ', 'in', 'support', 'of', 'its', 'contention', 'of', 'the', 'correct', 'assessment', 'the', 'board', 'of', 'review', 'submitted', 'eight', 'sales', 'comparables', '.', '  '], 'target': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(full.examples[150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 189053\n",
    "train_data, test_data, valid_data = full.split(split_ratio=[0.7, 0.15, 0.15], random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 31921\n",
      "Number of validation examples: 6840\n",
      "Number of testing examples: 6841\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 4533\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "#13114 unique @ min_freq 15\n",
    "#8700 unique @ min_freq 25\n",
    "#6186 unique @ min_freq 35\n",
    "#4533 unique @ min_freq 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = 'cpu',\n",
    "    sort_key=lambda x: len(x.facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docket': None,\n",
       " 'outcome': None,\n",
       " 'facts': <torchtext.data.field.Field at 0x2db05bfe0c8>,\n",
       " 'conclusion': None,\n",
       " 'target': <torchtext.data.field.LabelField at 0x2db05bfe088>}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_iterator.dataset.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Return accuracy per batch\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "class WordEmbAvg(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)  \n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()                                 \n",
    "        \n",
    "    def forward(self, text):\n",
    "        embeddings = self.embedding(text)\n",
    "        embeddings_avg = embeddings.mean(0)\n",
    "        output = self.linear1(embeddings_avg)\n",
    "        final = self.linear2(self.relu(output))\n",
    "        return final\n",
    "    \n",
    "class Training_module( ):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.model.train()\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        '''\n",
    "        Train the model for one epoch. For this repeat the following, \n",
    "        going through all training examples.\n",
    "        1. Get the next batch of inputs from the iterator.\n",
    "        2. Determine the predictions using a forward pass.\n",
    "        3. Compute the loss.\n",
    "        4. Compute gradients using a backward pass.\n",
    "        5. Execute one step of the optimizer to update the model paramters.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        for batch in iterator:\n",
    "            #batch.facts has the texts and batch.target has the labels.          \n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(batch.facts).squeeze(1)\n",
    "            loss = self.loss_fn(predictions, batch.target)                      \n",
    "            accuracy = binary_accuracy(predictions, batch.target) \n",
    "                   \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "                        \n",
    "            #print(accuracy.item())\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def train_model(self, train_iterator, dev_iterator):\n",
    "        \"\"\"\n",
    "        Train the model for multiple epochs, and after each evaluate on the\n",
    "        development set.  Return the best performing model.\n",
    "        \"\"\"  \n",
    "        dev_accs = [0.]\n",
    "        for epoch in range(5):\n",
    "            self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(f\"Epoch {epoch}: Dev Accuracy: {dev_acc[1]} Dev Loss:{dev_acc[0]}\")\n",
    "            if dev_acc[1] > max(dev_accs):\n",
    "                best_model = copy.deepcopy(self)\n",
    "            dev_accs.append(dev_acc[1])\n",
    "        return best_model.model\n",
    "                \n",
    "    def evaluate(self, iterator):\n",
    "        '''\n",
    "        Evaluate the performance of the model on the given examples.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                predictions = self.model(batch.facts).squeeze(1)\n",
    "                loss = self.loss_fn(predictions, batch.target)                      \n",
    "                accuracy = binary_accuracy(predictions, batch.target) \n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += accuracy.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = WordEmbAvg(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Dev Accuracy: 0.8342373165014748 Dev Loss:0.390515882138894\n",
      "Epoch 1: Dev Accuracy: 0.8516980972245475 Dev Loss:0.3515528648673931\n",
      "Epoch 2: Dev Accuracy: 0.8563084112149533 Dev Loss:0.3362796686102297\n",
      "Epoch 3: Dev Accuracy: 0.860877002511069 Dev Loss:0.3281265132338087\n",
      "Epoch 4: Dev Accuracy: 0.8659671227508616 Dev Loss:0.32171097627588524\n"
     ]
    }
   ],
   "source": [
    "model = model.to()\n",
    "tm = Training_module(model)\n",
    "\n",
    "#Training the model\n",
    "best_model = tm.train_model(train_iterator, valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.329 | Test Acc: 86.08%\n"
     ]
    }
   ],
   "source": [
    "tm.model = best_model\n",
    "test_loss, test_acc = tm.evaluate(test_iterator)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tm.model.embedding.weight.data\n",
    "result = list(torch.norm(a, p=2, dim=1).numpy())\n",
    "\n",
    "word_ls = TEXT.vocab.itos\n",
    "top_n = 10\n",
    "top_neg = {}\n",
    "top_pos = {}\n",
    "\n",
    "max_pos = [result.index(x) for x in sorted(result, reverse=True)][:top_n]\n",
    "max_neg = [result.index(x) for x in sorted(result)][:top_n]\n",
    "\n",
    "for i in range(top_n):\n",
    "    i_p = max_pos[i]\n",
    "    i_n = max_neg[i]\n",
    "    top_pos[word_ls[i_p]] = result[i_p]\n",
    "    top_neg[word_ls[i_n]] = result[i_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0.0,\n",
       " 'improvements': 7.49217,\n",
       " 'kleszynski': 7.693131,\n",
       " '1914': 7.777946,\n",
       " '565,000': 7.804003,\n",
       " 'county.1the': 7.8282547,\n",
       " '1.75-story': 7.8316336,\n",
       " 'arlington': 7.839141,\n",
       " 'assessors': 7.8665643,\n",
       " 'aux': 7.8999124}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kpp/6': 14.356398,\n",
       " 'limited': 13.905575,\n",
       " 'submit': 13.670415,\n",
       " 'valuation': 13.214185,\n",
       " 'timely': 13.147738,\n",
       " 'fourequity': 13.143389,\n",
       " 'establishing': 13.016701,\n",
       " 'insupport': 12.967831,\n",
       " 'applying': 12.952833,\n",
       " '.25': 12.9120245}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
