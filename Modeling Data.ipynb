{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#based on homework 2 framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('standardized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(\n",
    "    sequential=True,\n",
    "    fix_length=500,\n",
    "    tokenize='spacy',\n",
    "    pad_first=True,\n",
    "    lower=True\n",
    ")\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float,\n",
    "                        use_vocab=False, \n",
    "                        sequential=False,\n",
    "                        is_target=True)\n",
    "\n",
    "full = data.TabularDataset('standardized.csv', 'csv', skip_header=True,\n",
    "        fields=[\n",
    "            ('docket', None),\n",
    "            ('outcome', None),\n",
    "            ('facts', TEXT),\n",
    "            ('conclusion', None),\n",
    "            ('target', LABEL)\n",
    "        ])\n",
    "\n",
    "TEXT.build_vocab(\n",
    "    full,\n",
    "    max_size=20000,\n",
    "    min_freq=100,\n",
    "    vectors=None\n",
    ")\n",
    "\n",
    "LABEL.build_vocab(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'facts': ['  ', 'the', 'subject', 'property', 'consists', 'of', 'a', '6,250', 'parcel', 'of', 'land', 'improved', 'with', 'a', '95-year', 'old', ',', 'one', '-', 'story', ',', 'commercial', 'building', 'containing', '3,178', 'square', 'feet', 'of', 'building', 'area', '.', ' ', 'the', 'property', 'is', 'located', 'in', 'chicago', ',', 'jefferson', 'township', ',', 'cook', 'county', '.', ' ', 'the', 'property', 'is', 'a', 'docket', 'no', ':', '09', '-', '24022.001-c-1', '   ', '2', 'of', '4', 'class', '5', 'property', 'under', 'the', 'cook', 'county', 'real', 'property', 'assessment', 'classification', 'ordinance', '.', '  ', 'the', 'appellant', 'contends', 'overvaluation', 'as', 'the', 'basis', 'of', 'the', 'appeal', '.', 'in', 'support', 'of', 'this', 'argument', 'the', 'appellant', 'submitted', 'an', 'appraisal', 'estimating', 'the', 'subject', 'property', 'had', 'a', 'market', 'value', 'of', '$', '238,000', 'as', 'of', 'january', '1', ',', '2009', '.', '  ', 'the', 'board', 'of', 'review', 'submitted', 'its', '\"', 'board', 'of', 'review', 'notes', 'on', 'appeal', '\"', 'disclosing', 'the', 'total', 'assessment', 'for', 'the', 'subject', 'of', '$', '83,421', '.', ' ', 'the', 'subject', \"'s\", 'assessment', 'reflects', 'a', 'market', 'value', 'of', '$', '333,684', 'using', 'the', 'cook', 'county', 'ordinance', 'level', 'of', 'assessment', 'for', 'class', '5', 'property', 'of', '25', '%', '.', ' ', 'in', 'support', 'of', 'its', 'contention', 'of', 'the', 'correct', 'assessment', 'the', 'board', 'of', 'review', 'submitted', 'eight', 'sales', 'comparables', '.', '  '], 'target': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(full.examples[150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 189053\n",
    "train_data, test_data, valid_data = full.split(split_ratio=[0.7, 0.15, 0.15], random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 31921\n",
      "Number of validation examples: 6840\n",
      "Number of testing examples: 6841\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 2799\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "#13114 unique @ min_freq 15\n",
    "#8700 unique @ min_freq 25\n",
    "#6186 unique @ min_freq 35\n",
    "#4533 unique @ min_freq 50\n",
    "#3352 unique @ min_freq 75\n",
    "#2799 unique @ min_freq 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = 'cpu',\n",
    "    sort_key=lambda x: len(x.facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docket': None,\n",
       " 'outcome': None,\n",
       " 'facts': <torchtext.data.field.Field at 0x2dc1f7df388>,\n",
       " 'conclusion': None,\n",
       " 'target': <torchtext.data.field.LabelField at 0x2dc28679588>}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_iterator.dataset.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Return accuracy per batch\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "class WordEmbAvg(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)  \n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()                                 \n",
    "        \n",
    "    def forward(self, text):\n",
    "        embeddings = self.embedding(text)\n",
    "        embeddings_avg = embeddings.mean(0)\n",
    "        output = self.linear1(embeddings_avg)\n",
    "        final = self.linear2(self.relu(output))\n",
    "        return final\n",
    "    \n",
    "class Training_module( ):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.model.train()\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        '''\n",
    "        Train the model for one epoch. For this repeat the following, \n",
    "        going through all training examples.\n",
    "        1. Get the next batch of inputs from the iterator.\n",
    "        2. Determine the predictions using a forward pass.\n",
    "        3. Compute the loss.\n",
    "        4. Compute gradients using a backward pass.\n",
    "        5. Execute one step of the optimizer to update the model paramters.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        for batch in iterator:\n",
    "            #batch.facts has the texts and batch.target has the labels.          \n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(batch.facts).squeeze(1)\n",
    "            loss = self.loss_fn(predictions, batch.target)                      \n",
    "            accuracy = binary_accuracy(predictions, batch.target) \n",
    "                   \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "                        \n",
    "            #print(accuracy.item())\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def train_model(self, train_iterator, dev_iterator):\n",
    "        \"\"\"\n",
    "        Train the model for multiple epochs, and after each evaluate on the\n",
    "        development set.  Return the best performing model.\n",
    "        \"\"\"  \n",
    "        dev_accs = [0.]\n",
    "        for epoch in range(5):\n",
    "            self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(f\"Epoch {epoch}: Dev Accuracy: {dev_acc[1]} Dev Loss:{dev_acc[0]}\")\n",
    "            if dev_acc[1] > max(dev_accs):\n",
    "                best_model = copy.deepcopy(self)\n",
    "            dev_accs.append(dev_acc[1])\n",
    "        return best_model.model\n",
    "                \n",
    "    def evaluate(self, iterator):\n",
    "        '''\n",
    "        Evaluate the performance of the model on the given examples.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                predictions = self.model(batch.facts).squeeze(1)\n",
    "                loss = self.loss_fn(predictions, batch.target)                      \n",
    "                accuracy = binary_accuracy(predictions, batch.target) \n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += accuracy.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = WordEmbAvg(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Dev Accuracy: 0.830816087878753 Dev Loss:0.4032839011644649\n",
      "Epoch 1: Dev Accuracy: 0.848819259171174 Dev Loss:0.35916098353461684\n",
      "Epoch 2: Dev Accuracy: 0.8560372162088056 Dev Loss:0.3438979541169149\n",
      "Epoch 3: Dev Accuracy: 0.859062082856615 Dev Loss:0.3351214023951058\n",
      "Epoch 4: Dev Accuracy: 0.8627336448598131 Dev Loss:0.32859058538886987\n"
     ]
    }
   ],
   "source": [
    "model = model.to()\n",
    "tm = Training_module(model)\n",
    "\n",
    "#Training the model\n",
    "best_model = tm.train_model(train_iterator, valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.333 | Test Acc: 86.04%\n"
     ]
    }
   ],
   "source": [
    "tm.model = best_model\n",
    "test_loss, test_acc = tm.evaluate(test_iterator)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tm.model.embedding.weight.data\n",
    "result = list(torch.norm(a, p=2, dim=1).numpy())\n",
    "\n",
    "word_ls = TEXT.vocab.itos\n",
    "top_n = 25\n",
    "top_neg = {}\n",
    "top_pos = {}\n",
    "\n",
    "max_pos = [result.index(x) for x in sorted(result, reverse=True)][:top_n]\n",
    "max_neg = [result.index(x) for x in sorted(result)][:top_n]\n",
    "\n",
    "for i in range(top_n):\n",
    "    i_p = max_pos[i]\n",
    "    i_n = max_neg[i]\n",
    "    top_pos[word_ls[i_p]] = result[i_p]\n",
    "    top_neg[word_ls[i_n]] = result[i_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0.0,\n",
       " 'landassessment': 7.8902884,\n",
       " 'nos': 8.041609,\n",
       " '1971': 8.113046,\n",
       " 'joliet': 8.1292515,\n",
       " 'which': 8.129589,\n",
       " 'condominiums': 8.170181,\n",
       " 'percent': 8.186706,\n",
       " '24': 8.204296,\n",
       " 'with': 8.207816,\n",
       " 'regards': 8.239091,\n",
       " 'propertyhas': 8.249024,\n",
       " '    ': 8.27115,\n",
       " 'iii': 8.284628,\n",
       " '.15': 8.309838,\n",
       " '315,000': 8.311799,\n",
       " 'sheet': 8.382522,\n",
       " 'english': 8.401081,\n",
       " '23,000': 8.402858,\n",
       " 'auto': 8.418486,\n",
       " '352': 8.433661,\n",
       " '58-year': 8.436627,\n",
       " 'skokie': 8.441336,\n",
       " '1,232': 8.450788,\n",
       " '520': 8.46168}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'submit': 14.341018,\n",
       " 'estimating': 13.694119,\n",
       " 'valuation': 13.666112,\n",
       " 'rebuttal': 13.294349,\n",
       " 'fourequity': 13.2800455,\n",
       " 'insupport': 13.203797,\n",
       " 'limited': 12.991287,\n",
       " 'frankfort': 12.96951,\n",
       " 'purported': 12.940401,\n",
       " 'correctly': 12.932102,\n",
       " 'aurora': 12.825233,\n",
       " '105': 12.789168,\n",
       " 'stipulate': 12.7762,\n",
       " 'comparables1': 12.71342,\n",
       " 'proposed': 12.635103,\n",
       " 'but': 12.51256,\n",
       " 'rendered': 12.4135895,\n",
       " 'meadows': 12.37192,\n",
       " 'applying': 12.351024,\n",
       " 'comparables.in': 12.338044,\n",
       " 'complete': 12.327074,\n",
       " 'described': 12.323813,\n",
       " 'matter': 12.322682,\n",
       " 'madison': 12.310271,\n",
       " 'effect': 12.308301}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
